{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vastdb.session import Session\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "vast_db_access = os.getenv(\"VAST_DB_ACCESS\")\n",
    "vast_db_secret = os.getenv(\"VAST_DB_SECRET\")\n",
    "endpoint = os.getenv(\"VAST_DB_ENDPOINT\")\n",
    "sess = Session(access=vast_db_access, secret=vast_db_secret, endpoint=endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = spark.table(\"ndb.colabfit.dev.po_wip\")\n",
    "cos = spark.table(\"ndb.colabfit.dev.co_dataset_id_sets\")\n",
    "# css = spark.table('ndb.colabfit.dev.cs_wip')\n",
    "dss = spark.table(\"ndb.colabfit.dev.ds_wip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "ocs = dss.select(\"id\", \"name\").filter(sf.col(\"name\").contains(\"OC20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in [x[\"id\"] for x in ocs.collect()]:\n",
    "    with open(\"OC20_cos_dataset_ids2.txt\", \"a\") as f:\n",
    "        f.write(f\"{id}\\n\\n\")\n",
    "    cos_ds_ids = cos.filter(sf.col(\"dataset_ids\").contains(id)).select(\"dataset_ids\")\n",
    "    cos_ds_ids = cos_ds_ids.distinct().collect()\n",
    "    with open(\"OC20_cos_dataset_ids2.txt\", \"a\") as f:\n",
    "        for x in cos_ds_ids:\n",
    "            f.write(f'{x[\"dataset_ids\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following ids can be removed when we fix OC20 -- all instances are shared only between the existing OC20\n",
    "# datasets, whether S2EF or IS2RES\n",
    "# +-----------------+----------------------+\n",
    "# |id               |name                  |\n",
    "# +-----------------+----------------------+\n",
    "# |DS_zdy2xz6y88nl_0|OC20_S2EF_train_200K  |\n",
    "# |DS_dgop3abwq9ya_0|OC20_IS2RES_train     |\n",
    "# |DS_7qi6dh0ig7sd_0|OC20_S2EF_train_2M    |\n",
    "# |DS_otx1qc9f3pm4_0|OC20_S2EF_train_20M   |\n",
    "# |DS_wmgdq06mzdys_0|OC20_S2EF_val_ood_cat |\n",
    "# |DS_cgdsc3gxoamu_0|OC20_S2EF_val_ood_ads |\n",
    "# |DS_wv9zv6egp9vk_0|OC20_S2EF_val_id      |\n",
    "# |DS_889euoe7akyy_0|OC20_S2EF_val_ood_both|\n",
    "# +-----------------+----------------------+\n",
    "#  in addition, the id DS_rf10ovxd13ne_0 is one that should be removed as a failed ingest\n",
    "#  of the 20 M split of S2EF from mongo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from ast import literal_eval\n",
    "\n",
    "unstring_and_make_set_list_udf = sf.udf(\n",
    "    lambda x: str(list(set(literal_eval(x))), StringType())\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfdb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
